{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab32aa66",
   "metadata": {},
   "source": [
    "# Research Project Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "226f2cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import snntorch as snn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c457afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader arguments\n",
    "batch_size = 128 \n",
    "data_path='./data/mnist'\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "## if you're on M1 or M2 GPU:\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4fc48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.KMNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.KMNIST(data_path, train=False, download=True, transform=transform)\n",
    "# datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "972d02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "# train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "# test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee591d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "num_inputs = 28*28\n",
    "# num_hidden = 500 # vary\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "# num_steps = 25 # vary\n",
    "# beta = 0.95 # vary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51b5a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_hidden, new_beta, num_steps):\n",
    "    # def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=new_beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif2 = snn.Leaky(beta=new_beta)\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden states at t=0\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        \n",
    "        # Record the final layer\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        for step in range(self.num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0), torch.stack(mem2_rec, dim=0)\n",
    "        \n",
    "# Load the network onto CUDA if available\n",
    "# net = Net(500, 0.95).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed62ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "# num_epochs = 1\n",
    "# loss_hist = []\n",
    "# test_loss_hist = []\n",
    "# counter = 0\n",
    "\n",
    "# # Outer training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_batch = iter(train_loader)\n",
    "\n",
    "#     # Minibatch training loop\n",
    "#     for data, targets in train_batch:\n",
    "#         data = data.to(device)\n",
    "#         targets = targets.to(device)\n",
    "\n",
    "#         # forward pass\n",
    "#         net.train()\n",
    "#         spk_rec, _ = net(data.flatten(1))\n",
    "\n",
    "#         # initialize the loss & sum over time\n",
    "#         loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "#         loss_val += loss(spk_rec.sum(0), targets)\n",
    "\n",
    "#         # Gradient calculation + weight update\n",
    "#         optimizer.zero_grad()\n",
    "#         loss_val.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Store loss history for future plotting\n",
    "#         loss_hist.append(loss_val.item())\n",
    "\n",
    "#         # Print train/test loss/accuracy\n",
    "#         if counter % 10 == 0:\n",
    "#             print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
    "#         counter += 1\n",
    "\n",
    "#         if counter == 50:\n",
    "#           break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c941ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_accuracy(model, dataloader):\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    running_length = 0\n",
    "    running_accuracy = 0\n",
    "\n",
    "    for data, targets in iter(dataloader):\n",
    "      data = data.to(device)\n",
    "      targets = targets.to(device)\n",
    "\n",
    "      # forward-pass\n",
    "      spk_rec, _ = model(data.flatten(1))\n",
    "      spike_count = spk_rec.sum(0)\n",
    "      _, max_spike = spike_count.max(1)\n",
    "\n",
    "      # correct classes for one batch\n",
    "      num_correct = (max_spike == targets).sum()\n",
    "\n",
    "      # total accuracy\n",
    "      running_length += len(targets)\n",
    "      running_accuracy += num_correct\n",
    "    \n",
    "    accuracy = (running_accuracy / running_length)\n",
    "\n",
    "    return accuracy.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9dc5fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Test set accuracy: {measure_accuracy(net, test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ad86a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cummulate_loss_history = []\n",
    "\n",
    "def calc_fitness(batch_size, num_hidden, beta, num_steps):\n",
    "    train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    net = Net(num_hidden, beta, num_steps).to(device)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    # optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
    "\n",
    "    num_epochs = 1\n",
    "    loss_hist = []\n",
    "    test_loss_hist = []\n",
    "    counter = 0\n",
    "\n",
    "    # Outer training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_batch = iter(train_loader)\n",
    "\n",
    "        # Minibatch training loop\n",
    "        for data, targets in train_batch:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            net.train()\n",
    "            spk_rec, _ = net(data.flatten(1))\n",
    "\n",
    "            # initialize the loss & sum over time\n",
    "            loss_val = torch.zeros((1), dtype=dtype, device=device)\n",
    "            loss_val += loss(spk_rec.sum(0), targets)\n",
    "\n",
    "            # Gradient calculation + weight update\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store loss history for future plotting\n",
    "            loss_hist.append(loss_val.item())\n",
    "\n",
    "            # Print train/test loss/accuracy\n",
    "            # if counter % 10 == 0:\n",
    "            #     print(f\"Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
    "            counter += 1\n",
    "\n",
    "            # if counter == 50:\n",
    "            #     break\n",
    "    \n",
    "    cummulate_loss_history.append(loss_hist)\n",
    "    return measure_accuracy(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d17a65e-8e2b-473e-b34f-db00a424db49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# CS 420/CS 527 Lab 2: Genetic Algorithms in LEAP \n",
    "# Author: Catherine Schuman\n",
    "# February 2022\n",
    "\n",
    "evaluation_log = []  # Global list to store rows for dataframe\n",
    "current_gen = 0\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from toolz import pipe\n",
    "\n",
    "from leap_ec import Individual, context, test_env_var\n",
    "from leap_ec import ops, probe, util\n",
    "from leap_ec.decoder import IdentityDecoder\n",
    "from leap_ec.binary_rep.problems import MaxOnes\n",
    "from leap_ec.binary_rep.initializers import create_binary_sequence\n",
    "from leap_ec.binary_rep.ops import mutate_bitflip\n",
    "from leap_ec.binary_rep.problems import ScalarProblem\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "# Implementation of a custom problem\n",
    "class Lab2Problem(ScalarProblem):\n",
    "    def __init__(self):\n",
    "        super().__init__(maximize=True)\n",
    "        \n",
    "    def evaluate(self, ind):\n",
    "        \n",
    "        binary_str = ''.join(['1' if b else '0' for b in ind])\n",
    "\n",
    "        # Bit slices\n",
    "        batch_bits   = int(binary_str[0:3], 2)    # 3 bits = 0–7\n",
    "        hidden_bits  = int(binary_str[3:9], 2)    # 6 bits = 0–63\n",
    "        beta_bits    = int(binary_str[9:14], 2)   # 5 bits = 0–31\n",
    "        steps_bits   = int(binary_str[14:17], 2)  # 3 bits = 0–7\n",
    "\n",
    "        # Map to real hyperparameters\n",
    "        batch_size  = 64 + (batch_bits % 7) * 32         # 64, 96, ..., 256\n",
    "        num_hidden = 200 + (hidden_bits % 41) * 20      # 200–1000\n",
    "        beta        = round(0.5 + (beta_bits / 31) * (0.99 - 0.5), 4)  # 0.5–0.99\n",
    "        num_steps   = 10 + (steps_bits % 5) * 10\n",
    "\n",
    "        accuracy = calc_fitness(batch_size=batch_size, num_hidden=num_hidden, beta=beta, num_steps=num_steps)\n",
    "\n",
    "        evaluation_log.append({\n",
    "            # \"generation\": context.generation,\n",
    "            \"generation\": current_gen,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"num_hidden\": num_hidden,\n",
    "            \"beta\": beta,\n",
    "            \"num_steps\": num_steps,\n",
    "            \"accuracy\": accuracy\n",
    "        })\n",
    "\n",
    "        # return calc_fitness()\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description=\"Lab 2: Genetic Algorithms\")\n",
    "#     parser.add_argument(\"--n\", default=50, help=\"population size\", type=int)\n",
    "#     parser.add_argument(\"--p_m\", default=0.01, help=\"probability of mutation\", type=float)\n",
    "#     parser.add_argument(\"--p_c\", default=0.3, help=\"probability of crossover\", type=float)\n",
    "#     parser.add_argument(\"--trn_size\", default=2, help=\"tournament size\", type=int)\n",
    "#     parser.add_argument(\"--csv_output\", required=True, help=\"csv output file name\", type=str)\n",
    "#     args = parser.parse_args()    \n",
    "\n",
    "N = 10\n",
    "p_m = 0.01\n",
    "p_c = 0.3\n",
    "trn_size = 3\n",
    "\n",
    "max_generation = 15\n",
    "l = 17\n",
    "parents = Individual.create_population(N,\n",
    "                                       initialize=create_binary_sequence(\n",
    "                                           l),\n",
    "                                       decoder=IdentityDecoder(),\n",
    "                                       problem=Lab2Problem())\n",
    "\n",
    "# Evaluate initial population\n",
    "parents = Individual.evaluate_population(parents)\n",
    "\n",
    "generation_counter = util.inc_generation()\n",
    "out_f = open(\"best.txt\", \"w\")\n",
    "\n",
    "while generation_counter.generation() < max_generation:\n",
    "    print(current_gen)\n",
    "    offspring = pipe(parents,\n",
    "                     ops.tournament_selection(k=trn_size),\n",
    "                     ops.clone,\n",
    "                     mutate_bitflip(probability=p_m),\n",
    "                     ops.UniformCrossover(p_xover=p_c),\n",
    "                     ops.evaluate,\n",
    "                     ops.pool(size=len(parents)),  # accumulate offspring\n",
    "                     probe.AttributesCSVProbe(stream=out_f, do_fitness=True)\n",
    "                    )\n",
    "    \n",
    "    parents = offspring\n",
    "    generation_counter()  # increment to the next generation\n",
    "    current_gen += 1\n",
    "\n",
    "out_f.close()\n",
    "df = pd.DataFrame(evaluation_log)\n",
    "df.to_csv(\"evolution_log.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dff4ec2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(cummulate_loss_history)\n",
    "df.to_csv(\"cummulate_loss_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ef12437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"cummulate_loss_history_JSON.json\", \"w\") as f:\n",
    "    json.dump(cummulate_loss_history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aa4f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cummulate_loss_history_JSON.json\", \"r\") as f:\n",
    "    history = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5484c8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array_equal(cummulate_loss_history, history)\n",
    "cummulate_loss_history == history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ded741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_list = pd.read_csv(\"cummulate_loss_history.csv\")\n",
    "# history2 = df_list.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acd6654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.array_equal(cummulate_loss_history, history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f9438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# history2 == history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"evolution_log.csv\")\n",
    "# plot best accuracy per generation\n",
    "best_by_gen = df.groupby(\"generation\")[\"accuracy\"].max()\n",
    "plt.plot(best_by_gen.index, best_by_gen.values)\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Best Accuracy (%)\")\n",
    "plt.title(\"Evolution Progress\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3805462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call fitness function for both models (when training logic is finished inside it)\n",
    "baseline_acc = calc_fitness(\n",
    "    batch_size=128,\n",
    "    num_hidden=500,\n",
    "    beta=0.95,\n",
    "    num_steps=25\n",
    ")\n",
    "\n",
    "evolved_acc = calc_fitness(\n",
    "    batch_size=128,\n",
    "    num_hidden=650,  # example evolved values\n",
    "    beta=0.90,\n",
    "    num_steps=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14367be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([\"Baseline\", \"Evolved\"], [baseline_acc * 100, evolved_acc * 100])\n",
    "plt.ylabel(\"Test Accuracy (%)\")\n",
    "plt.title(\"Comparison of Accuracy\")\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deae7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SNN Practice with MNIST and Rate Encoding\n",
    "# import torch\n",
    "# from torchvision import datasets, transforms\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Step 1: Load one MNIST image (as grayscale 28x28)\n",
    "# transform = transforms.Compose([transforms.ToTensor()])\n",
    "# mnist = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# image, label = mnist[0]  # Just take the first image for example\n",
    "# image = image.squeeze()  # Remove extra channel dimension (1, 28, 28) -> (28, 28)\n",
    "\n",
    "# # Step 2: Show the original MNIST image\n",
    "# plt.imshow(image, cmap='gray')\n",
    "# plt.title(f\"Original MNIST Image - Label: {label}\")\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# # Step 3: Normalize pixel values to [0, 1] to use as firing probabilities\n",
    "# normalized_image = image.numpy()  # Convert tensor to numpy array\n",
    "# normalized_image = normalized_image / normalized_image.max()  # Ensure values between 0 and 1\n",
    "\n",
    "# # Step 4: Encode to spikes over T time steps\n",
    "# T = 100  # Total simulation time steps\n",
    "# spike_train = np.zeros((T, 28, 28))\n",
    "\n",
    "# for t in range(T):\n",
    "#     # Flip a coin for each pixel: will it spike at time t?\n",
    "#     spikes = np.random.rand(28, 28) < normalized_image\n",
    "#     spike_train[t] = spikes\n",
    "\n",
    "# # Step 5: Visualize spike activity over time for one pixel\n",
    "# pixel_row, pixel_col = 14, 14  # Pick center pixel\n",
    "# pixel_spike_times = spike_train[:, pixel_row, pixel_col]\n",
    "\n",
    "# print(f\"Spikes over time for pixel ({pixel_row},{pixel_col}):\")\n",
    "# print(pixel_spike_times.astype(int))\n",
    "\n",
    "# # Step 6: Plot total number of spikes at each time step (activity curve)\n",
    "# total_spikes = spike_train.sum(axis=(1, 2))\n",
    "# plt.figure()\n",
    "# plt.plot(range(T), total_spikes)\n",
    "# plt.xlabel(\"Time step\")\n",
    "# plt.ylabel(\"Total spikes\")\n",
    "# plt.title(\"Spiking Activity Over Time\")\n",
    "# plt.show()\n",
    "\n",
    "# # Step 7: Show a spike raster plot for a small 5x5 patch of pixels\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# for i in range(5):\n",
    "#     for j in range(5):\n",
    "#         spikes = spike_train[:, 10+i, 10+j]\n",
    "#         spike_times = np.where(spikes == 1)[0]\n",
    "#         plt.scatter(spike_times, np.ones_like(spike_times) * (i*5 + j), s=10)\n",
    "\n",
    "# plt.xlabel(\"Time step\")\n",
    "# plt.ylabel(\"Pixel Index (5x5 Patch)\")\n",
    "# plt.title(\"Spike Raster Plot for 5x5 Pixels\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e78dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
